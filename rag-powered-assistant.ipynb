{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-powered Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Powered Assistant for PDF or Text Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Goal:\n",
    "\n",
    "Build a small app that answers questions from a document or a knowledge base using¬†RAG (Retrieval-Augmented Generation).\n",
    "The pipeline:¬†embed ‚Üí store ‚Üí retrieve ‚Üí generate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz\n",
    "from huggingface_hub import InferenceClient\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pdf (math chapter)\n",
    "file = \"./data/mathematical-concepts.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 20640 characters from PDF.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = file\n",
    "# Load the PDF file\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "print(f\"Extracted {len(text)} characters from PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Text into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 42 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500 \n",
    "chunks = wrap(text, chunk_size)\n",
    "print(f\"Document split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (42, 384)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = [embedding_model.encode(chunk) for chunk in chunks]\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "print(f\"Generated embeddings with shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Embeddings in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 42 chunks.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"FAISS index created with\", index.ntotal, \"chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, top_k=3):\n",
    "    query_emb = embedding_model.encode(query)\n",
    "    query_emb = np.array([query_emb], dtype=\"float32\")\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    return [(chunks[i], distances[0][pos]) for pos, i in enumerate(indices[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient of a scalar function is a vector that points in the direction of the steepest increase of the function and indicates the rate of change of the function with respect to position. It is calculated as the vector of partial derivatives of the function with respect to each coordinate. For example, if ùëì(ùë•,ùë¶,ùëß) represents temperature, its gradient points in the direction of fastest temperature increase. In Cartesian coordinates, the gradient is expressed as ‚àáùëì = (‚àÇùëì/‚àÇùë•)√Æ + (‚àÇùëì/‚àÇùë¶)ƒµ + (‚àÇùëì/‚àÇùëß)ùëòÃÇ. The gradient is fundamental in vector calculus and applications such as electromagnetism and heat transfer.\n"
     ]
    }
   ],
   "source": [
    "client = InferenceClient(api_key=hf_api_key)\n",
    "\n",
    "def answer_question(query):\n",
    "    # Retrieve context\n",
    "    relevant_chunks = search(query, top_k=3)\n",
    "    context = \"\\n\".join([chunk for chunk, _ in relevant_chunks])\n",
    "\n",
    "    prompt = f\"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "                1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "                2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "                3. Use ONLY the following context to answer the question. If the answer is not contained in the context, respond with \"I don't know.\"   \n",
    "\n",
    "                {context}\n",
    "\n",
    "                Question: {query}\n",
    "\n",
    "                Helpful Answer:\"\"\"\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"{query}\"}\n",
    "        ]\n",
    "\n",
    "    # Call Hugging Face Inference API\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example\n",
    "print(answer_question(\"What is a gradient\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
